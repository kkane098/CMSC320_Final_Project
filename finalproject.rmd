---
title: "CMSC320 Final Project"
author: "Kevin Kane, Matthew Sinnott"
date: "May 7, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include = FALSE}
read_csv <- "[read_csv()](https://readr.tidyverse.org/reference/read_delim.html)"

select <- "[select()](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/select)"
mutate <- "[mutate()](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/mutate)"
filter <- "[filter()](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/filter)"
factor <- "[factor()](https://www.rdocumentation.org/packages/base/versions/3.6.0/topics/factor)"
levels <- "[levels()](https://www.rdocumentation.org/packages/base/versions/3.6.0/topics/levels)"
table <- "[table()](https://www.rdocumentation.org/packages/base/versions/3.6.0/topics/table)"
lm <- "[lm()](https://www.rdocumentation.org/packages/stats/versions/3.6.0/topics/lm)"
glance <- "[glance()](https://www.rdocumentation.org/packages/broom/versions/0.4.4/topics/glance)"
tidy <- "[tidy()](https://www.rdocumentation.org/packages/broom/versions/0.4.4/topics/tidy)"
augment <- "[augment()](https://www.rdocumentation.org/packages/broom/versions/0.4.4/topics/augment)"
geom_violin <- "[geom_violin()](https://www.rdocumentation.org/packages/ggplot2/versions/0.9.1/topics/geom_violin)"
median <- "[median()](https://www.rdocumentation.org/packages/stats/versions/3.6.0/topics/median)"
quantile <- "[quantile()](https://www.rdocumentation.org/packages/stats/versions/3.6.0/topics/quantile)"
log10 <- "[log10()](https://www.rdocumentation.org/packages/base/versions/3.6.0/topics/log)"

str_detect <- "[str_detect()](https://www.rdocumentation.org/packages/stringr/versions/1.4.0/topics/str_detect)"
str_replace <- "[str_replace()](https://www.rdocumentation.org/packages/stringr/versions/1.4.0/topics/str_replace)"
as.double <- "[as.double()](https://www.rdocumentation.org/packages/base/versions/3.6.0/topics/double)"
```

## Introduction:
This tutorial will walk you through many of the major aspects of data science, including data cleaning, exploratory data analysis, hypothesis testing, machine learning, and curation of a message. We will be using the R programming language, along with the tidyverse library in order to ingest, read, process and display our data, methods and results.

We wanted to use data science to examine Airbnb data from San Fransisco. We were wondering about how the price of a listing corresponds with the listing's various attributes - its location, number of bedrooms, type of lodging, etc. There are many factors that impact the price, including some that we don't have access to, like images on the listing page, but we wanted to see how well we could model and predict listing prices based solely on quantitative attributes.

To begin, let's examine the dataset we will be using throughout this tutorial. To download the data we are using, visit [here](http://insideairbnb.com/get-the-data.html) and click on the listing.csv.gz link.  The csv can be loaded into R as follows.

```{r load_csv, warning=FALSE, message=FALSE}
library(tidyverse)
dat <- read_csv("listings_sf_all.csv")
head(dat)
```

Note that this code will only work if it is run in the same directory as the csv, otherwise you need to pass `r read_csv` the full filepath to where the csv is stored.

## Part 1 - Cleaning:
Often, as is the case here, when you load your data you will need to do some extra processing to make it easier to use in later analysis. This step is commonly referred to as **cleaning**.  

For example, this dataset has all the information you would expect to find in an AirBnb listing, but much of that information, such as URL data and text descriptions, is not especially useful for analysis. So, a good first step is to eliminate unnecessary columns to make the data a little easier to read and work with. A good tool to do this is the `r select` function, which can be passed a list of columns from a data frame and will return a new data frame with only those columns.  The following `r select` call should leave us with only the useful data.

```{r select_dat}
selected_dat <- dat %>% 
  select(host_id, host_since, host_response_time, host_response_rate, 
         host_is_superhost, host_listings_count, host_identity_verified, 
         neighbourhood_cleansed, zipcode, latitude, longitude, 
         id, property_type, room_type, accommodates, bathrooms, bedrooms, beds, bed_type, amenities, square_feet, 
         price, weekly_price, monthly_price, security_deposit, cleaning_fee, guests_included, extra_people, 
         minimum_nights, maximum_nights, availability_30, availability_60, availability_90, availability_365, 
         number_of_reviews, number_of_reviews_ltm, review_scores_rating, reviews_per_month)

selected_dat %>% head()
```

As you can see, this new data frame is much cleaner and will be easier to work with going forward.

However, we are far from done with cleaning. A closer inspection of some of the columns reveals a new issue.

```{r explanation_1}
selected_dat %>% 
  select(amenities, price, weekly_price, monthly_price) %>%
  head()
```

Many of the columns in our dataset are in forms that make them tricky to deal with.  For example, all the prices are still stored as strings, when we would really like them to be stored as doubles.  Additionally, the amenities column has a lot of useful information about the listing, but that information is difficult to use as is.

Let's start by extracting the useful data from the amenities column.  A key function we'll use here is the `r mutate` function, which stores the result of an operation preformed on a column in a new column.  Combining `r mutate` with the `r str_detect` function allows us to create several new logical columns that represent the data contained in the amenities column in a way that will make it much easier to use later. Having transferred all the useful data from the amenities column, we can also now safely remove it.

```{r fix_amenities}
fixed_amenities <- selected_dat %>% 
  mutate(has_internet = str_detect(amenities, "Wifi") || str_detect(amenities, "Internet")) %>%
  mutate(has_tv = str_detect(amenities, "TV")) %>%
  mutate(has_kitchen = str_detect(amenities, "Kitchen")) %>%
  mutate(has_washer = str_detect(amenities, "Washer")) %>%
  mutate(has_dryer = str_detect(amenities, "Dryer")) %>%
  mutate(has_heating = str_detect(amenities, "Heating")) %>%
  mutate(has_ac = str_detect(amenities, "Air conditioning")) %>% 
  mutate(pets_allowed = str_detect(amenities, "Pets allowed")) %>%
  select(-amenities)

fixed_amenities %>% 
  select(has_internet, has_tv, has_kitchen, has_washer, has_dryer, has_heating, has_ac, pets_allowed) %>%
  head()
```

Now let's take a look at the price columns.  Currently, prices are stored as strings and cannot be converted to doubles because of the presence of '$' and ',' characters.  However, these can be removed by calling `r str_replace` with the empty string.  After that, we can safely convert the price columns to doubles using `r as.double`.

```{r fix_prices}
fixed_prices <- fixed_amenities %>% 
  mutate(price            = as.double(str_replace(str_replace(price,            ',', ''), '\\$', ''))) %>%
  mutate(weekly_price     = as.double(str_replace(str_replace(weekly_price,     ',', ''), '\\$', ''))) %>%
  mutate(monthly_price    = as.double(str_replace(str_replace(monthly_price,    ',', ''), '\\$', ''))) %>%
  mutate(security_deposit = as.double(str_replace(str_replace(security_deposit, ',', ''), '\\$', ''))) %>%
  mutate(cleaning_fee     = as.double(str_replace(str_replace(cleaning_fee,     ',', ''), '\\$', ''))) %>%
  mutate(extra_people     = as.double(str_replace(str_replace(extra_people,     ',', ''), '\\$', '')))
```

However, there is still a problem with the prices. 

```{r explanation_2}
fixed_prices %>% 
  select(price, weekly_price, monthly_price, security_deposit, cleaning_fee, extra_people) %>% 
  head()
```

As you can see, several prices are encoded as 'NA' in the table.  This is referred to as **missing data** and there are several ways to deal with it.  General options are to replace the missing data with the mean value of the column or the predicted value from some model.  However, in this case we can use our knowledge of the data to replace the missing values.  The weekly_price and monthly_price columns represent special rates provided by the owner of the listing for visitors with longer stays.  If they are 'NA', that means that there is no discount and those missing values can be replaced by the nightly price (the price column) multiplied by 7 or 30 respectively.  We can also safely assume that if the security_deposit or cleaning_fee columns are 'NA' that there is no required security deposit or cleaning fee, so those missing values can be replaced with 0.

```{r fix_prices_2}
fixed_prices <- fixed_prices %>%
  mutate(weekly_price     = ifelse(is.na(weekly_price),     price*7, weekly_price)) %>%
  mutate(monthly_price    = ifelse(is.na(monthly_price),    price*30, monthly_price)) %>%
  mutate(security_deposit = ifelse(is.na(security_deposit), 0, security_deposit)) %>%
  mutate(cleaning_fee     = ifelse(is.na(cleaning_fee),     0, cleaning_fee))

fixed_prices %>% 
  select(price, weekly_price, monthly_price, security_deposit, cleaning_fee, extra_people) %>%
  head()
```

Another thing that will be helpful later is encoding the columns that represent categorical variables (variables that can take on a value from a small set of values) as factors. Specifically, we want to ensure that our model will treat variables like zipcode and bedrooms as categorical variables rather than continuous numbers. We can do this using the `r factor` function.

```{r make_factors}
factor_dat <- fixed_prices %>% 
  mutate(property_type = factor(property_type)) %>%
  mutate(room_type = factor(room_type)) %>%
  mutate(zipcode = factor(zipcode)) %>%
  mutate(neighbourhood = factor(neighbourhood_cleansed)) %>%
  mutate(host_response_time = factor(host_response_time)) %>%
  mutate(bed_type = factor(bed_type)) %>%
  mutate(bedrooms = factor(bedrooms)) %>%
  mutate(bathrooms = factor(bathrooms)) %>%
  mutate(beds = factor(beds)) %>%
  mutate(accommodates = factor(accommodates)) %>%
  select(-neighbourhood_cleansed)

factor_dat %>% 
  select(property_type, room_type, zipcode, neighbourhood, host_response_time, bed_type, bedrooms, bathrooms, beds, accommodates) %>%
  head()
```

Storing these columns as factors also allows us to easily check the range of values stored in each column using the `r levels` function. We can also easily view the number of rows associated with each value using the `r table` function.

```{r explanation_3}
levels(factor_dat$property_type)

table(factor_dat$bedrooms)
```

For more information on factors in R and why they are useful check [here](https://www.stat.berkeley.edu/~s133/factors.html).


```{r fix_hosts}
host_fixed_dat <- factor_dat %>%
  mutate(host_response_rate = ifelse(is.na(host_response_rate), '0%', host_response_rate)) %>%
  mutate(host_response_rate = ifelse(host_response_rate == 'N/A', '0%', host_response_rate)) %>%
  mutate(host_response_rate = 0.01 * as.double(str_replace(host_response_rate, '%', '')))

host_fixed_dat %>% select(host_response_rate) %>% head()
```

## EDA

First, we do some simple graphs to get a better feel for the dataset. The first graph we plot is of the distribution of prices. As you can see from the violin plot (`r geom_violin`), the data is heavily skewed - the vast majority of prices are quite low, but there is a long tail of increasing prices going all the way up to $10,000. 

```{r log_dat, message= FALSE, warning= FALSE}
host_fixed_dat %>% 
  ggplot(aes(x="", y = price)) + 
  geom_violin() + 
  labs(title = "Distribution of Price", y = "Price", x = "")
```

Skew is calculated by how far the `r median` is from the average of the first and third `r quantile` of the data. When calculating the skew for the price, we get a skew of \$35, which is quite significant given that the average price for an accommodation is \$213. Additionally, we can see that the median is also very different from the mean - they differ by almost $65 dollars. This high degree of skew makes the data difficult to work with and graph.
```{r} 
host_fixed_dat %>% 
  summarize(mean_price = mean(price))

skew_dat <- host_fixed_dat %>% 
  summarise(median = median(price), first_q = quantile(price, 1/4), third_q = quantile(price, 3/4)) %>%
  mutate(skew = (median - first_q) - (third_q - median))

skew_dat
```

In order to learn more about the skew, we took a closer look at the properties with cost greater than \$5000. There are only 5 of them in a dataset of more than 7,000, making them significant outliers. In order to get a better sense of what these data points were, we joined in the listing url to allow us to look at the whole listing. Because there are only 5 listings, we went through them individually so that we could find out what was happening. All 5 of these price values are incorrect - the first, second and fourth are 550, 60 and 1100 respectively, while the third and fifth listings are no longer available. Given that these listings are all in error, we can safely remove them from the dataset.

```{r}
host_fixed_dat %>% 
  filter(price >= 5000) %>%
  select(id, price, number_of_reviews) %>%
  inner_join(dat %>% select(id, listing_url), by=c("id"))

clean_dat <- host_fixed_dat %>% filter(price < 5000)
```

However, even though the major outliers have been cleaned, there is still quite a range of valid prices - a little over 3 orders of magnitude. The data still has a long tail of higher prices, which makes graphs difficult to read and interpret. Using a standard linear scale packs the vast majority of the data into a small slice near the bottom and leaves a lot of the upper graph blank. In order to make the graphs easier to interpret, we use a logarithmic scale (using the base 10 logarithmic function `r log10`). A log transformation reduces skew, so the resulting graph is easier to read and analyze visually.
```{r}
log_price_dat <- clean_dat %>% filter(price > 0) %>% mutate(log_price = log10(price))

log_skew_dat <- log_price_dat %>% summarise(median = median(log_price), 
                                             first_q = quantile(log_price, 1/4), 
                                             third_q = quantile(log_price, 3/4)) %>% 
  mutate(skew = (median - first_q) - (third_q - median))

log_skew_dat

log_price_dat %>% 
  ggplot(aes(x="", y = log_price)) + 
  geom_boxplot() + 
  labs(title = "Distribution of Transformed Price", y = "Price (logbase10)", x = "")
```

We now want to do some Elementary Data Analysis. First, we plot our log(price) data against neighborhood in order to determine visually if price is based on neighborhood (we'll calculate this mathematically later).  This plot gives us some useful insights about certain neighborhoods. For example, Golden Gate Park has a very tightly clustered distribution with a couple of outliers. Presidio has no outliers, just a single cluster at a single price. This suggests that there are not that many listings in these locations and that the listings that are there are likely coordinated. In contrast, South of Market has a ton of outliers and a very small box portion of the box plot. The small box describes a heavily clustered dataset, but the lack of dropoff suggests other factors at play in the price other than location. Overall, the graph doesn't show an obvious trend regarding location, but some neighborhoods trend higher than others. Marina is noticeably more pricey than some other options, and Crocker Amazon is obviously cheaper.

```{r boxplots1, message= FALSE, warning= FALSE}
log_price_dat %>% 
  ggplot(aes(x = neighbourhood, y = log_price, group = neighbourhood)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title = "Distribution of Price by Neighbourhood", y = "Price (logbase10)", x = "Neighbourhood")
```

The graph of property type is much more varied. Some properties, such as Castle, Earth House and Treehouse only have 1 entry, making their bar plot a single line. Others are a bit more spread, but still clustered reasonable closely, such Boat, Cabin and Tiny House. The four of major interest however are Apartment, Condominium, Hotel and House. Hotel has a large cluster near the middle of its distribution and then a handful of outliers both above and below. We can see that hotel prices are controlled more closely than the others. Condos and houses both have the same "long tail" up towards higher prices, but apartment's tails go both above and below the mean, as well as are more populated with data.
```{r boxplots2, message= FALSE, warning= FALSE}
log_price_dat %>% 
  ggplot(aes(x = property_type, y = log_price, group = property_type)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title = "Distribution of Price by Property Type", y = "Price (logbase10)", x = "Property Type")
```

Regarding room type the price differences are even more clear. Sharing a room is (quite understandably) seen as a detriment, while renting the entire house costs noticeably more. Having a private room falls somewhere in the middle. Again, we see the tails of higher prices, which notably follow the trend of the majority of the data set - the price difference carries across the board.
```{r boxplots3, message= FALSE, warning= FALSE}
log_price_dat %>% 
  ggplot(aes(x = room_type, y = log_price, group = room_type)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title = "Distribution of Price by Room Type", y = "Price (logbase10)", x = "Room Type")
```

While some of the other metrics are luxuries, or at least involve some choice on the part of the renter, the number of bedrooms is usually a parameter of the search. The average price as a function of number of bedrooms looks linear by inspection (0 bedrooms corresponds to studio apartments, which usually have a pull-out bed, but no bedroom). We also notice that there is 1 place with 14 bedrooms that doesn't follow the trend, as well as a couple of N/A values, but the number of these is insignificant as compared with the size of the entire dataset. We'll go into more detail on this later.
```{r boxplots4, message= FALSE, warning= FALSE}
log_price_dat %>% 
  ggplot(aes(x = bedrooms, y = log_price, group = bedrooms)) + 
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title = "Distribution of Price by Number of Bedrooms", y = "Price (logbase10)", x = "Bedrooms")
```

Finally, we look at the impact of air conditioning. Given that the data is from California Airbnbs, AC is quite likely an important factor in people's choice of where to live. We see that places with AC are priced higher than places without, on average, but there are far more places without AC than with it. Additionally, the places with AC are more clustered - they have less outliers.
```{r boxplots5, message= FALSE, warning= FALSE}
log_price_dat %>% ggplot(aes(x = has_ac, y = log_price)) + 
  geom_boxplot() +
  labs(title = "Distribution of Price Based on Whether Listing has Air Conditioning", y = "Price (logbase10)", x = "Has AC?")

```

##Linear Modeling
So, the work we did in EDA allowed us to examine general trends in how variables like number of bedrooms affect the price.  However, we cannot really make any conclusive statements about the quality of any these variables as predictors of price with the information we have so far.  This is where linear modeling comes in.  A **linear model** essentially refers an equation of the form $Y = B_0 + B_1X_1+ B_2X_2 ...$ representing a linear combination of independent variables used to predict the value of a single dependent variable (in this case price).  Using the `r lm` function, we can train a linear model to learn the appropriate values of the coefficients in the linear combination (the $B_i$s in the equation above).  We can then use functions from the broom library such as `r tidy`, `r glance`, and `r augment` to view information about the created model.  A more detailed explanation of linear models can be found [here](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm).

```{r model1}
library(broom)
model1 <- lm(log_price~bedrooms+bathrooms, data = log_price_dat)
model1_stats <- model1 %>% glance()

model1_stats
```

In the code above, we model the log of the price of a listing using only the number of bedrooms and bathrooms.  However, a look at the output of the `r glance` function tells us this model is not very good.  The $R^2$ value is a measure of how well the model fits the data.  In a good model, we want this value to be as close to 1 as possible.  However, the fact that this model's $R^2$ value is so low is not that surprising, there are obviously many more factors that affect the price of a listing than just the number of bedrooms and bathrooms.  So let's try a model that incorporates more variables.

```{r model2}
model2 <- lm(log_price~bedrooms+bathrooms+beds+accommodates+room_type+property_type+neighbourhood+has_ac+has_tv+has_washer+has_dryer+has_internet+has_kitchen+has_heating+pets_allowed+review_scores_rating, data = log_price_dat)
model2_stats <-model2 %>% glance()

model2_stats
```

As expected, this model, which uses almost all of the useful columns from the dataset, is much better.  But do we really need all of these variables?  In general, in order to make your model as clear as possible you only want to include the variables that have a clear effect on the output variable.  This is where an element of **hypothesis testing** comes into play.  In this case, the hypothesis we want to test is whether a specific variable has any impact on the price.  However, this is difficult to prove directly, so we instead make use of a **null hypothesis**, which in this case is that a specific variable has no effect on the price.  If the probability of the null hypothesis being true is low (conventionally <.05), then we can reject it, meaning we can say with high probability the variable has an effect on the price.  Conveniently, the `r tidy` function includes the probabilities associated with the null hypothesis for each variable.

```{r}
model2_coeffs <- model2 %>% tidy()

model2_coeffs %>% head()
```

So let's examine the variables for which we cannot reject the null hypothesis.

```{r}
library(knitr)
model2_coeffs %>% filter(p.value > .05) %>% kable()
```

We clearly cannot say anything meaningful about the effect the presence of a washer, dryer, or kitchen has on price.  However, analysis of the other variables is a little trickier, as the other variables all represent portions of a single categorical variable.  So how do we handle categorical variables for which we can only reject the null hypothesis for some of the domain? Here, it is best to do so on a case by case basis.  For property_type, neighborhood, and accommodates, the values for which we cannot reject the null hypothesis represent a relatively small portion of the domain, so discarding these variables entirely would likely be unwise.  For beds and bathrooms, it is best to examine the distribution of listings across the domain.

```{r}
table(log_price_dat$bathrooms)
table(log_price_dat$beds)
```

As you can see, the values of bathrooms for which we cannot reject the null hypothesis represent a very small number of listings.  In contrast, the values of beds for which we cannot reject the null hypothesis represent a large portion of listings.  As such, we should still include bathrooms in our model, whereas beds can likely be disregarded.

So let's examine a linear model without beds, has_kitchen, has_washer, and has_dryer.

```{r model 3}
model3 <- lm(log_price~bedrooms+bathrooms+accommodates+room_type+property_type+neighbourhood+has_ac+has_tv+pets_allowed+review_scores_rating, data = log_price_dat)
model3_coeffs <- model3 %>% tidy()
model3_stats <- model3 %>% glance()
model3_aug <- model3 %>% augment()

model3_stats
```

Even though we discarded four variables, the new model's $R^2$ value is less than .001 lower! This means this smaller model will be much better to use going forward.

However, before we can move on and analyze the coefficients in this new model, there is one last thing we should do.  In order to verify our assumption of a linear relationship between the variables and price, we need to ensure that there are no trends in the residuals (the difference between the predicted value and the observed value).  If our model is sound, the residuals should be uniformly and randomly clustered around 0 when plotted against the fitted value (the log price).

```{r}
model3_aug %>% ggplot(aes(x=.fitted,y=.resid)) + geom_point() + geom_smooth(method = "loess") + labs(x="Fitted", y="Residual")
```

As the above graph meets the listed requirements, our model is sound and can now be analyzed appropriately.

Since we are using the log of the price, interpreting the learned coefficients can be a bit tricky.  Normally, if the term "bedrooms1" had a coefficient of 100, we could say that having 1 bedrooms increases the price of a listing by \$100 (over having no bedrooms).  However, in this case the relationship is more multiplicative.  As is detailed [here](https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/), the best way to interpret the coefficients our model is to calculate a percent change in the price elicited by the presence of a variable by taking $(10^{estimate} - 1) * 100$.
```{r}
interpreted_model <- model3_coeffs %>% mutate(pct_change = (10^estimate - 1) * 100)
```

By doing this we can gain insight into the degree different variables affect the price.

```{r}
interpreted_model %>% filter(str_detect(term, "bedrooms")) %>% select(term, pct_change)
```

For example, we can see that the presence of one bedroom increses the value of a listing by 15%, whereas the presence of 5 bedrooms increases the value by nearly 200%! This kind of analysis is not perfect however, as we are left with puzzling percent change values in some areas.  For example, this analysis would tell us the presence of 14 bedrooms decreases the value of a listing by nearly 75%, when in reality this number is derived from one extreme outlier.

## Conclusion

In summation, we can say with high probability that number of bedrooms, number of bathrooms, number people accomodated, property type, room type, neighborhood, air conditioning, TV, pet friendliness, and rating all affect the price of a listing.  If we were to revisit this analysis, we would likely try nonlinear models in order to better fit the data, better address outliers (either via better scraping or outlier detection), and perhaps expand the listings we analyzed to outside of just San Fransisco to get a larger sample size and see what we can say about AirBnb listings as a whole.